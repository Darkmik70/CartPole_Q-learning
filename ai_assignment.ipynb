{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6a8e7c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 16 - Cart Pole Q-learning\n",
    "### Author: Michał Krępa 6475383\n",
    "\n",
    "This project is a solution to the assignment given during AI For Robotics I course at UniGe.\n",
    "The project number is 16.\n",
    "\n",
    "## Project Structure\n",
    "The project consists of two 3 components:\n",
    "- `CartPole`: A wrapper class for OpenAI Gymnasium CartPole environment\n",
    "- `Q_learnign`: Agent which implements Qlearnig algorithm\n",
    "\n",
    "- `main.py`: Main function with all the variables\n",
    "\n",
    "## Requirements\n",
    "To successfully run the project few dependencies are needed.\n",
    "\n",
    "- `gymnasium` - Library from OpenAI containing API with required Environments\n",
    "- `numpy` -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4dc2e3a-01e3-44e8-b662-6b9874143ec6",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (from gymnasium) (1.24.3)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (from gymnasium) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: numpy in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d46979-dd94-47ed-8eaf-9bcfbe306b6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b39881-4fb6-460b-9261-513432e40921",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CartPole class\n",
    "This is a wrapper class for  Gymnasium's CartPole environmnent. The reason I had in mind while creating this, was to follow the example given during lab sessions. While creating it I spotted the main challenge I had with this project - The Environment generates a continuous observation space, which later on has to be turned discrete, if we want to have some efficient computations about it. After some struggle, I ended up with the observation space digitized into 10 pieces.\n",
    "\n",
    "Additionally this wrapper function has some member functions that are meant to return specific values, more details in the code.\n",
    "\n",
    "The object of this class can be initialized using the flag of `is_learning` set to `False` to get the visualizations of the CartPole environment. Nevertheless, this is not recommended while training the model, as it may prolong the training time effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6d1643-dc6c-475e-a3cd-b2c48f12c98b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CartPole:\n",
    "    \"\"\"\n",
    "        Wrapper class for CartPole environment\n",
    "\n",
    "        Attributes:\n",
    "            _env: The Gym environment for the Cart Pole game.\n",
    "            _curr_state (np.array): The current state of the environment.\n",
    "            _isTerminated (bool): Flag indicating whether the current episode has ended.\n",
    "    \"\"\"\n",
    "    def __init__(self, is_learning = False):\n",
    "        \"\"\"\n",
    "        Initializes the CartPole environment\n",
    "\n",
    "        Args:\n",
    "            is_learning (bool): Flag to determine if the environment is for learning or visualization.\n",
    "        \"\"\"\n",
    "        # Define whether we want to visualize\n",
    "        if is_learning:\n",
    "            self._env = gym.make('CartPole-v1')\n",
    "        else:\n",
    "            self._env = gym.make('CartPole-v1', render_mode = \"human\")\n",
    "        self._currState = self._env.reset()[0]\n",
    "        self._isTerminated = False\n",
    "\n",
    "\n",
    "    def digitize_state(self, state):\n",
    "        \"\"\"\n",
    "        Digitizes the continuous state into discrete values for Q-table.\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            list: A list representing the digitized state.\n",
    "        \"\"\"\n",
    "        pos_space = np.linspace(-2.4, 2.4, 10)\n",
    "        vel_space = np.linspace(-4, 4, 10)\n",
    "        ang_space = np.linspace(-.2095, .2095, 10)\n",
    "        ang_vel_space = np.linspace(-4, 4, 10)\n",
    "        \n",
    "        new_state_p = np.digitize(state[0], pos_space)\n",
    "        new_state_v = np.digitize(state[1], vel_space)\n",
    "        new_state_a = np.digitize(state[2], ang_space)\n",
    "        new_state_av= np.digitize(state[3], ang_vel_space)\n",
    "        new_state_dig = [new_state_p, new_state_v, new_state_a, new_state_av]\n",
    "        return new_state_dig\n",
    "\n",
    "    def do_action(self, action):\n",
    "       \"\"\"\n",
    "        Performs a step in the environment. Gets the values for Observation, reward and checks if the game is over\n",
    "\n",
    "        Args:\n",
    "            action (int): an action passed to the environment\n",
    "        Returns:\n",
    "            new_state: Discrete state after the action is taken\n",
    "            reward: Reward basing on the taken action\n",
    "       \"\"\"\n",
    "       new_state, reward, self._isTerminated, _, _ = self._env.step(action)       \n",
    "       # Update the current state\n",
    "       self._currState = new_state\n",
    "       return self.digitize_state(new_state), reward\n",
    "    \n",
    "    def reset_env(self):\n",
    "        \"\"\" Resets the environment \"\"\"\n",
    "        self._currState = self._env.reset()[0]\n",
    "        self._isTerminated = False\n",
    "\n",
    "    def get_current_state(self):\n",
    "        \"\"\" Gets the discrete state of the environment \"\"\"\n",
    "        return self.digitize_state(self._currState)\n",
    "    \n",
    "    def get_action_space(self):\n",
    "        \"\"\"Returns the size of the action space\"\"\"\n",
    "        return self._env.action_space.n\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        \"\"\" Returns boolean determining if game is over\"\"\"\n",
    "        return self._isTerminated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae1321-8bd5-435b-9b05-74fd1cff6693",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Q_learning Agent\n",
    "The class that contains the Q_learning agent follows, or at least attempts to follow the style shown during lab sessions. Inside this class there are few interesting things:\n",
    "\n",
    "- `_Q_table` - the object of Q-table is not a dictionary of dictionaries as it was done during the class, as I found it difficult to do that for a 4 dimensional state space (or observation space). I am also not that good with python, so basically tried to create a 4-dim array of lists + action space (which in our case is pretty simple. The cart can only go left or right, so two values). This object ends up being initialized as `11x11x11x11x2` array, which in my opinion is quite big, but thought of it as necessary to get rather precise results in the longer run.\n",
    "\n",
    "\n",
    "- **Policy** - For the policy I am using the *Epsilon Greedy Policy* with an additional modifications. I wanted to train my model, so added there a boolean for learning, to distinguish if we want to explore or just use the values from the Q-table\n",
    "\n",
    "- Epsilon Decay Rate - While doing my own research for this project I noticed that Epsilon Greedy Policy can be adjusted with additional use of the Epsilon that is decaying over the episodes, having first fully random choice, and then turning the use to rely more and more on the Q-table. I was unsure whether I could use this or not, but In the end it can be found in the code with a proper comments next to it. If the user would like to turn it of, they can simply comment that one line or remove it from the code\n",
    "\n",
    "```python\n",
    "            # Epsilon Decay rate \n",
    "            self._epsilon = self._epsilon - self._decayRate\n",
    "```\n",
    "Where decay rate is calculated as:\n",
    "\n",
    "```python\n",
    "        self._decayRate = epsilon / episodes\n",
    "```\n",
    "\n",
    "- `apply` - this function does the whole simulation. It works in two modes. Depending on `isLearning` flag, the first mode is meant for learning, second one is meant to use the Q-table to \"show off\" the capabilities of trained model.\n",
    "if we run the agent with `isLearning` set to `True` at the end we will receive an object of `Q_table.pkl `. This is the filled Q-table that will be loaded when `isLearning` will be set to `False`.  Finally this function also provides some data for statistics to see whether our model is learning efficiently or not. Every 100 episodes it prints out the message as in the example below:\n",
    "```\n",
    "Episode: 81500 Rewards: 142.0  Epsilon: 0.44  Mean Rewards 80.8\n",
    "```\n",
    "Where Episode - is the current episode number, Rewards go for the obtained rewards for that current episode, Epsilon shows the current value of the Epsilone and Mean rewards showing the average score for 100 episodes.\n",
    "\n",
    "**Additionally there is a threshold for learning that will quit the process once the mean score is igger than 1000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebece168-ad20-47c6-8aa9-7ffe84d76855",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Q_learning:\n",
    "    \"\"\"\n",
    "        Implementation of Q-learning algorhitm for the CartPole environment.\n",
    "\n",
    "        Attributes:\n",
    "            _env (cartPoleEnv): Cart Pole env\n",
    "            _gamma (float):   The discount factor\n",
    "            _alpha (float): The learning rate.\n",
    "            _epsilon (float): The exploration rate.\n",
    "            _episodes (int): The number of episodes for training\n",
    "            _is_learning (bool): Flag indicating whether the agent is in learning mode.\n",
    "            _Q_table (np.array): The Q-table, stores state-action values\n",
    "    \"\"\"\n",
    "    def __init__(self, env, gamma, alpha, epsilon, episodes, isLearning = True):\n",
    "        \"\"\"\n",
    "            Initializes Q-learning agent.\n",
    "\n",
    "            Works in two ways. When isLearning flag is set True,\n",
    "            it initializes Q-table as a empty np.array, else it tries to load it from file.\n",
    "            Args:\n",
    "                env (cartPoleEnv): The Cart Pole environment.\n",
    "                gamma (float): The discount factor.\n",
    "                alpha (float): The learning rate.\n",
    "                epsilon (float): The exploration rate.\n",
    "                episodes (int): The number of episodes for training.\n",
    "                isLearning (bool): Flag to determine if the agent is in learning mode.\n",
    "        \"\"\"\n",
    "        self._env = env\n",
    "        self._gamma = gamma\n",
    "        self._alpha = alpha\n",
    "        self._epsilon = epsilon\n",
    "        self._episodes = episodes\n",
    "        self._isLearning = isLearning\n",
    "        self._decayRate = epsilon / episodes\n",
    "\n",
    "        if self._isLearning:\n",
    "            print(f'Learning mode on: training agent on alpha: {self._alpha}, gamma: {self._gamma}, epsilon : {self._epsilon}, with {self._episodes} episodes')\n",
    "        else:\n",
    "            print('Visualization mode on')\n",
    "\n",
    "        # Initialize Q_Table\n",
    "        if self._isLearning: \n",
    "            # State is given as continuous set of variables\n",
    "            # we need to cut it into pieces to be able to learn\n",
    "            # The limits here are the limits for our game to be over\n",
    "            pos_space = np.linspace(-2.4, 2.4, 10)\n",
    "            vel_space = np.linspace(-4, 4, 10)\n",
    "            ang_space = np.linspace(-.2095, .2095, 10) #value in rad\n",
    "            ang_vel_space = np.linspace(-4, 4, 10)\n",
    "            self.Q_table = np.zeros((len(pos_space)+1, len(vel_space)+1, \n",
    "                                    len(ang_space)+1, len(ang_vel_space)+1, self._env.get_action_space())) #11x11x11x11x2\n",
    "        else:\n",
    "            #Load the model\n",
    "            f = open('Q_table.pkl', 'rb')\n",
    "            self.Q_table = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\" \n",
    "        Epsilon Greedy Policy\n",
    "\n",
    "        Function works in two modes:\n",
    "            If isLearning is True, decides on random whether to choose random action or\n",
    "            the best action according to the Q_table. The higher epsilon, the higher chance of getting random results\n",
    "            When isLearning is set to False, policy only chooses the values basing on the Q_table.\n",
    "        \n",
    "        Args:\n",
    "            state: Discrete state of the environment\n",
    "        \"\"\"\n",
    "        if self._isLearning and np.random.random() < self._epsilon:\n",
    "            # Choose an action at random with probability epsilon\n",
    "            return random.choice([0,1]) # only two actions - left or right\n",
    "        else:\n",
    "            # Choose the best action accordin to Q_table with probability 1-epsilon\n",
    "            return np.argmax(self.Q_table[state[0], state[1], state[2], state[3], :])\n",
    "\n",
    "    def apply(self):\n",
    "        \"\"\"\n",
    "        Executes Q-learning algorhithm over a specified number of episodes.\n",
    "\n",
    "        This method runs the Q-learning algorithm, updating the Q-table based on the interactions\n",
    "        with the environment. It implements an epsilon-greedy policy for action selection and applies \n",
    "        temporal difference learning for updating the Q-table.\n",
    "        Additionally, the method also handles epsilon decay.\n",
    "         \n",
    "        For exploration over time and prints out the progress every 100 episodes.\n",
    "\n",
    "        The method performs the following steps in each episode:\n",
    "        - Interacts with the environment to obtain states, rewards, and new states.\n",
    "        - Updates the Q-table using the temporal difference\n",
    "        - Applies epsilon decay to gradually shift from exploration to exploitation.\n",
    "        - Tracks and logs the rewards for each episode.\n",
    "\n",
    "        At the end of the training, the updated Q-table is saved to a file (if in learning mode), \n",
    "        and the average reward across all episodes is calculated and printed to the output.\n",
    "        \"\"\"\n",
    "\n",
    "        total_episode_rewards = []  # Rewards of all runs\n",
    "        for episode in range(self._episodes):\n",
    "            episode_rewards = [] # rewards for each episode\n",
    "            rewards = 0\n",
    "            while not self._env.is_game_over():\n",
    "                # get the current state\n",
    "                curr_state = self._env.get_current_state()\n",
    "                action = self.policy(curr_state)\n",
    "                next_state, reward = self._env.do_action(action)\n",
    "                # Choose maximum Q-value for next state\n",
    "                max_next_value = np.max(self.Q_table[next_state[0], next_state[1], next_state[2], next_state[3], :])\n",
    "                # Temporal difference update TODO improve readability\n",
    "                self.Q_table[curr_state[0], curr_state[1], curr_state[2], curr_state[3], action] = self.Q_table[curr_state[0], curr_state[1], curr_state[2], curr_state[3], action] +\\\n",
    "                self._alpha * ( reward + self._gamma * max_next_value -  self.Q_table[curr_state[0], curr_state[1], curr_state[2], curr_state[3], action]) \n",
    "                rewards += reward\n",
    "\n",
    "            # Reset before new episode\n",
    "            self._env.reset_env()\n",
    "\n",
    "            # Epsilon Decay rate \n",
    "            self._epsilon = self._epsilon - self._decayRate\n",
    "            \n",
    "            # Get episode  rewards\n",
    "            total_episode_rewards.append(rewards)\n",
    "            mean_rewards = np.mean(total_episode_rewards[len(total_episode_rewards)-100:])\n",
    "            \n",
    "            if not self._isLearning:\n",
    "                # Display results after each episode\n",
    "                print(f'Episode: {episode} Rewards: {rewards}')\n",
    "            else:\n",
    "                # For every 100 display rewards\n",
    "                if episode % 100 == 0:\n",
    "                    print(f'Episode: {episode} Rewards: {rewards}  Epsilon: {self._epsilon:0.2f}  Mean Rewards {mean_rewards:0.1f}')\n",
    "                    total_episode_rewards.append(np.sum(episode_rewards))\n",
    "            \n",
    "            # Threshold for rewards\n",
    "            if mean_rewards >= 1000:\n",
    "                print(f' Mean rewards: {mean_rewards} - no need to train model longer')\n",
    "                break\n",
    "        \n",
    "        # Save Q table to file\n",
    "        if self._isLearning:\n",
    "            f = open('Q_table.pkl','wb')\n",
    "            pickle.dump(self.Q_table, f)\n",
    "            f.close()\n",
    "\n",
    "        # Calculate the mean\n",
    "        print(\"Average reward after all episodes: \", np.mean(total_episode_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3681a5-4be5-4bf5-af4a-408f7a30047d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The main function\n",
    "Finally, the main function defines the constant variables as `gamma` `alpha` `epsilon` or the amount of `episodes`.\n",
    "For this project I had chosen a big amount of episodes, justifying it by the size of the Q-table. A huge object like that will require many steps and episodes to be eventually filled and therefore bigger amount of steps.\n",
    "\n",
    "If I were to decide, this is probably not the best setup for this environment and Q-learning, as I was playing with different values I could get various results, sometimes having more than 100 000 episodes lead me to obtaining even +1k rewards for an episode, but in the end this works fine, after around 20 k we see the increase of average rewards and this continues till the end of simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b21e15-8874-4619-9e61-7061e18c77c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning mode on: training agent on alpha: 0.1, gamma: 0.7, epsilon : 0.5, with 40000 episodes\n",
      "Episode: 0 Rewards: 46.0  Epsilon: 0.50  Mean Rewards 46.0\n",
      "Episode: 100 Rewards: 43.0  Epsilon: 0.50  Mean Rewards 30.6\n",
      "Episode: 200 Rewards: 27.0  Epsilon: 0.50  Mean Rewards 25.5\n",
      "Episode: 300 Rewards: 58.0  Epsilon: 0.50  Mean Rewards 27.8\n",
      "Episode: 400 Rewards: 51.0  Epsilon: 0.49  Mean Rewards 26.8\n",
      "Episode: 500 Rewards: 47.0  Epsilon: 0.49  Mean Rewards 33.6\n",
      "Episode: 600 Rewards: 51.0  Epsilon: 0.49  Mean Rewards 35.0\n",
      "Episode: 700 Rewards: 48.0  Epsilon: 0.49  Mean Rewards 34.3\n",
      "Episode: 800 Rewards: 34.0  Epsilon: 0.49  Mean Rewards 38.6\n",
      "Episode: 900 Rewards: 16.0  Epsilon: 0.49  Mean Rewards 38.3\n",
      "Episode: 1000 Rewards: 10.0  Epsilon: 0.49  Mean Rewards 36.2\n",
      "Episode: 1100 Rewards: 20.0  Epsilon: 0.49  Mean Rewards 41.1\n",
      "Episode: 1200 Rewards: 31.0  Epsilon: 0.48  Mean Rewards 45.5\n",
      "Episode: 1300 Rewards: 24.0  Epsilon: 0.48  Mean Rewards 42.0\n",
      "Episode: 1400 Rewards: 35.0  Epsilon: 0.48  Mean Rewards 42.3\n",
      "Episode: 1500 Rewards: 33.0  Epsilon: 0.48  Mean Rewards 46.1\n",
      "Episode: 1600 Rewards: 72.0  Epsilon: 0.48  Mean Rewards 47.0\n",
      "Episode: 1700 Rewards: 91.0  Epsilon: 0.48  Mean Rewards 48.1\n",
      "Episode: 1800 Rewards: 70.0  Epsilon: 0.48  Mean Rewards 54.6\n",
      "Episode: 1900 Rewards: 100.0  Epsilon: 0.48  Mean Rewards 49.6\n",
      "Episode: 2000 Rewards: 25.0  Epsilon: 0.47  Mean Rewards 40.3\n",
      "Episode: 2100 Rewards: 16.0  Epsilon: 0.47  Mean Rewards 52.9\n",
      "Episode: 2200 Rewards: 15.0  Epsilon: 0.47  Mean Rewards 51.6\n",
      "Episode: 2300 Rewards: 15.0  Epsilon: 0.47  Mean Rewards 36.3\n",
      "Episode: 2400 Rewards: 17.0  Epsilon: 0.47  Mean Rewards 55.1\n",
      "Episode: 2500 Rewards: 53.0  Epsilon: 0.47  Mean Rewards 39.1\n",
      "Episode: 2600 Rewards: 32.0  Epsilon: 0.47  Mean Rewards 45.2\n",
      "Episode: 2700 Rewards: 26.0  Epsilon: 0.47  Mean Rewards 44.5\n",
      "Episode: 2800 Rewards: 30.0  Epsilon: 0.46  Mean Rewards 41.7\n",
      "Episode: 2900 Rewards: 68.0  Epsilon: 0.46  Mean Rewards 44.8\n",
      "Episode: 3000 Rewards: 62.0  Epsilon: 0.46  Mean Rewards 56.8\n",
      "Episode: 3100 Rewards: 17.0  Epsilon: 0.46  Mean Rewards 56.7\n",
      "Episode: 3200 Rewards: 93.0  Epsilon: 0.46  Mean Rewards 62.8\n",
      "Episode: 3300 Rewards: 22.0  Epsilon: 0.46  Mean Rewards 56.7\n",
      "Episode: 3400 Rewards: 48.0  Epsilon: 0.46  Mean Rewards 59.5\n",
      "Episode: 3500 Rewards: 48.0  Epsilon: 0.46  Mean Rewards 46.8\n",
      "Episode: 3600 Rewards: 10.0  Epsilon: 0.45  Mean Rewards 51.2\n",
      "Episode: 3700 Rewards: 44.0  Epsilon: 0.45  Mean Rewards 57.5\n",
      "Episode: 3800 Rewards: 44.0  Epsilon: 0.45  Mean Rewards 65.5\n",
      "Episode: 3900 Rewards: 58.0  Epsilon: 0.45  Mean Rewards 64.9\n",
      "Episode: 4000 Rewards: 83.0  Epsilon: 0.45  Mean Rewards 69.4\n",
      "Episode: 4100 Rewards: 73.0  Epsilon: 0.45  Mean Rewards 66.4\n",
      "Episode: 4200 Rewards: 92.0  Epsilon: 0.45  Mean Rewards 66.2\n",
      "Episode: 4300 Rewards: 56.0  Epsilon: 0.45  Mean Rewards 66.4\n",
      "Episode: 4400 Rewards: 74.0  Epsilon: 0.44  Mean Rewards 73.6\n",
      "Episode: 4500 Rewards: 48.0  Epsilon: 0.44  Mean Rewards 72.2\n",
      "Episode: 4600 Rewards: 35.0  Epsilon: 0.44  Mean Rewards 67.1\n",
      "Episode: 4700 Rewards: 96.0  Epsilon: 0.44  Mean Rewards 76.7\n",
      "Episode: 4800 Rewards: 89.0  Epsilon: 0.44  Mean Rewards 82.6\n",
      "Episode: 4900 Rewards: 129.0  Epsilon: 0.44  Mean Rewards 76.7\n",
      "Episode: 5000 Rewards: 88.0  Epsilon: 0.44  Mean Rewards 72.3\n",
      "Episode: 5100 Rewards: 241.0  Epsilon: 0.44  Mean Rewards 84.8\n",
      "Episode: 5200 Rewards: 70.0  Epsilon: 0.43  Mean Rewards 70.8\n",
      "Episode: 5300 Rewards: 88.0  Epsilon: 0.43  Mean Rewards 66.0\n",
      "Episode: 5400 Rewards: 12.0  Epsilon: 0.43  Mean Rewards 79.1\n",
      "Episode: 5500 Rewards: 170.0  Epsilon: 0.43  Mean Rewards 75.2\n",
      "Episode: 5600 Rewards: 106.0  Epsilon: 0.43  Mean Rewards 82.2\n",
      "Episode: 5700 Rewards: 105.0  Epsilon: 0.43  Mean Rewards 83.5\n",
      "Episode: 5800 Rewards: 108.0  Epsilon: 0.43  Mean Rewards 101.5\n",
      "Episode: 5900 Rewards: 165.0  Epsilon: 0.43  Mean Rewards 90.1\n",
      "Episode: 6000 Rewards: 22.0  Epsilon: 0.42  Mean Rewards 94.0\n",
      "Episode: 6100 Rewards: 106.0  Epsilon: 0.42  Mean Rewards 102.8\n",
      "Episode: 6200 Rewards: 118.0  Epsilon: 0.42  Mean Rewards 88.9\n",
      "Episode: 6300 Rewards: 97.0  Epsilon: 0.42  Mean Rewards 91.0\n",
      "Episode: 6400 Rewards: 110.0  Epsilon: 0.42  Mean Rewards 96.5\n",
      "Episode: 6500 Rewards: 38.0  Epsilon: 0.42  Mean Rewards 90.8\n",
      "Episode: 6600 Rewards: 135.0  Epsilon: 0.42  Mean Rewards 89.3\n",
      "Episode: 6700 Rewards: 108.0  Epsilon: 0.42  Mean Rewards 87.0\n",
      "Episode: 6800 Rewards: 62.0  Epsilon: 0.41  Mean Rewards 116.5\n",
      "Episode: 6900 Rewards: 86.0  Epsilon: 0.41  Mean Rewards 99.8\n",
      "Episode: 7000 Rewards: 182.0  Epsilon: 0.41  Mean Rewards 110.2\n",
      "Episode: 7100 Rewards: 122.0  Epsilon: 0.41  Mean Rewards 109.4\n",
      "Episode: 7200 Rewards: 118.0  Epsilon: 0.41  Mean Rewards 89.1\n",
      "Episode: 7300 Rewards: 193.0  Epsilon: 0.41  Mean Rewards 113.6\n",
      "Episode: 7400 Rewards: 183.0  Epsilon: 0.41  Mean Rewards 104.5\n",
      "Episode: 7500 Rewards: 142.0  Epsilon: 0.41  Mean Rewards 94.4\n",
      "Episode: 7600 Rewards: 156.0  Epsilon: 0.40  Mean Rewards 90.3\n",
      "Episode: 7700 Rewards: 114.0  Epsilon: 0.40  Mean Rewards 108.5\n",
      "Episode: 7800 Rewards: 91.0  Epsilon: 0.40  Mean Rewards 93.3\n",
      "Episode: 7900 Rewards: 183.0  Epsilon: 0.40  Mean Rewards 116.2\n",
      "Episode: 8000 Rewards: 142.0  Epsilon: 0.40  Mean Rewards 104.3\n",
      "Episode: 8100 Rewards: 142.0  Epsilon: 0.40  Mean Rewards 107.3\n",
      "Episode: 8200 Rewards: 114.0  Epsilon: 0.40  Mean Rewards 107.0\n",
      "Episode: 8300 Rewards: 194.0  Epsilon: 0.40  Mean Rewards 113.2\n",
      "Episode: 8400 Rewards: 73.0  Epsilon: 0.39  Mean Rewards 117.2\n",
      "Episode: 8500 Rewards: 65.0  Epsilon: 0.39  Mean Rewards 118.1\n",
      "Episode: 8600 Rewards: 58.0  Epsilon: 0.39  Mean Rewards 123.3\n",
      "Episode: 8700 Rewards: 178.0  Epsilon: 0.39  Mean Rewards 112.3\n",
      "Episode: 8800 Rewards: 177.0  Epsilon: 0.39  Mean Rewards 127.0\n",
      "Episode: 8900 Rewards: 178.0  Epsilon: 0.39  Mean Rewards 134.4\n",
      "Episode: 9000 Rewards: 31.0  Epsilon: 0.39  Mean Rewards 124.0\n",
      "Episode: 9100 Rewards: 65.0  Epsilon: 0.39  Mean Rewards 128.5\n",
      "Episode: 9200 Rewards: 62.0  Epsilon: 0.38  Mean Rewards 121.1\n",
      "Episode: 9300 Rewards: 173.0  Epsilon: 0.38  Mean Rewards 137.8\n",
      "Episode: 9400 Rewards: 227.0  Epsilon: 0.38  Mean Rewards 122.7\n",
      "Episode: 9500 Rewards: 116.0  Epsilon: 0.38  Mean Rewards 110.8\n",
      "Episode: 9600 Rewards: 169.0  Epsilon: 0.38  Mean Rewards 119.0\n",
      "Episode: 9700 Rewards: 195.0  Epsilon: 0.38  Mean Rewards 122.9\n",
      "Episode: 9800 Rewards: 96.0  Epsilon: 0.38  Mean Rewards 120.1\n",
      "Episode: 9900 Rewards: 113.0  Epsilon: 0.38  Mean Rewards 124.0\n",
      "Episode: 10000 Rewards: 166.0  Epsilon: 0.37  Mean Rewards 124.5\n",
      "Episode: 10100 Rewards: 86.0  Epsilon: 0.37  Mean Rewards 116.8\n",
      "Episode: 10200 Rewards: 118.0  Epsilon: 0.37  Mean Rewards 123.4\n",
      "Episode: 10300 Rewards: 186.0  Epsilon: 0.37  Mean Rewards 112.3\n",
      "Episode: 10400 Rewards: 107.0  Epsilon: 0.37  Mean Rewards 143.4\n",
      "Episode: 10500 Rewards: 112.0  Epsilon: 0.37  Mean Rewards 136.3\n",
      "Episode: 10600 Rewards: 50.0  Epsilon: 0.37  Mean Rewards 125.8\n",
      "Episode: 10700 Rewards: 94.0  Epsilon: 0.37  Mean Rewards 146.7\n",
      "Episode: 10800 Rewards: 161.0  Epsilon: 0.36  Mean Rewards 129.7\n",
      "Episode: 10900 Rewards: 135.0  Epsilon: 0.36  Mean Rewards 141.4\n",
      "Episode: 11000 Rewards: 101.0  Epsilon: 0.36  Mean Rewards 127.7\n",
      "Episode: 11100 Rewards: 18.0  Epsilon: 0.36  Mean Rewards 116.2\n",
      "Episode: 11200 Rewards: 179.0  Epsilon: 0.36  Mean Rewards 130.2\n",
      "Episode: 11300 Rewards: 42.0  Epsilon: 0.36  Mean Rewards 153.2\n",
      "Episode: 11400 Rewards: 129.0  Epsilon: 0.36  Mean Rewards 156.8\n",
      "Episode: 11500 Rewards: 185.0  Epsilon: 0.36  Mean Rewards 142.5\n",
      "Episode: 11600 Rewards: 149.0  Epsilon: 0.35  Mean Rewards 151.1\n",
      "Episode: 11700 Rewards: 30.0  Epsilon: 0.35  Mean Rewards 139.8\n",
      "Episode: 11800 Rewards: 235.0  Epsilon: 0.35  Mean Rewards 147.8\n",
      "Episode: 11900 Rewards: 164.0  Epsilon: 0.35  Mean Rewards 154.9\n",
      "Episode: 12000 Rewards: 313.0  Epsilon: 0.35  Mean Rewards 145.9\n",
      "Episode: 12100 Rewards: 50.0  Epsilon: 0.35  Mean Rewards 142.3\n",
      "Episode: 12200 Rewards: 121.0  Epsilon: 0.35  Mean Rewards 169.0\n",
      "Episode: 12300 Rewards: 254.0  Epsilon: 0.35  Mean Rewards 177.8\n",
      "Episode: 12400 Rewards: 143.0  Epsilon: 0.34  Mean Rewards 137.9\n",
      "Episode: 12500 Rewards: 180.0  Epsilon: 0.34  Mean Rewards 155.5\n",
      "Episode: 12600 Rewards: 184.0  Epsilon: 0.34  Mean Rewards 163.5\n",
      "Episode: 12700 Rewards: 99.0  Epsilon: 0.34  Mean Rewards 191.6\n",
      "Episode: 12800 Rewards: 105.0  Epsilon: 0.34  Mean Rewards 169.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 12900 Rewards: 136.0  Epsilon: 0.34  Mean Rewards 164.4\n",
      "Episode: 13000 Rewards: 163.0  Epsilon: 0.34  Mean Rewards 147.7\n",
      "Episode: 13100 Rewards: 225.0  Epsilon: 0.34  Mean Rewards 169.0\n",
      "Episode: 13200 Rewards: 204.0  Epsilon: 0.33  Mean Rewards 148.5\n",
      "Episode: 13300 Rewards: 190.0  Epsilon: 0.33  Mean Rewards 164.9\n",
      "Episode: 13400 Rewards: 107.0  Epsilon: 0.33  Mean Rewards 175.8\n",
      "Episode: 13500 Rewards: 193.0  Epsilon: 0.33  Mean Rewards 164.1\n",
      "Episode: 13600 Rewards: 177.0  Epsilon: 0.33  Mean Rewards 158.1\n",
      "Episode: 13700 Rewards: 25.0  Epsilon: 0.33  Mean Rewards 179.3\n",
      "Episode: 13800 Rewards: 174.0  Epsilon: 0.33  Mean Rewards 177.1\n",
      "Episode: 13900 Rewards: 251.0  Epsilon: 0.33  Mean Rewards 177.5\n",
      "Episode: 14000 Rewards: 156.0  Epsilon: 0.32  Mean Rewards 144.9\n",
      "Episode: 14100 Rewards: 260.0  Epsilon: 0.32  Mean Rewards 172.5\n",
      "Episode: 14200 Rewards: 233.0  Epsilon: 0.32  Mean Rewards 154.3\n",
      "Episode: 14300 Rewards: 167.0  Epsilon: 0.32  Mean Rewards 160.9\n",
      "Episode: 14400 Rewards: 104.0  Epsilon: 0.32  Mean Rewards 160.1\n",
      "Episode: 14500 Rewards: 130.0  Epsilon: 0.32  Mean Rewards 197.2\n",
      "Episode: 14600 Rewards: 165.0  Epsilon: 0.32  Mean Rewards 178.2\n",
      "Episode: 14700 Rewards: 59.0  Epsilon: 0.32  Mean Rewards 202.1\n",
      "Episode: 14800 Rewards: 274.0  Epsilon: 0.31  Mean Rewards 218.7\n",
      "Episode: 14900 Rewards: 195.0  Epsilon: 0.31  Mean Rewards 188.9\n",
      "Episode: 15000 Rewards: 125.0  Epsilon: 0.31  Mean Rewards 159.5\n",
      "Episode: 15100 Rewards: 224.0  Epsilon: 0.31  Mean Rewards 169.6\n",
      "Episode: 15200 Rewards: 162.0  Epsilon: 0.31  Mean Rewards 173.0\n",
      "Episode: 15300 Rewards: 168.0  Epsilon: 0.31  Mean Rewards 187.5\n",
      "Episode: 15400 Rewards: 269.0  Epsilon: 0.31  Mean Rewards 184.5\n",
      "Episode: 15500 Rewards: 179.0  Epsilon: 0.31  Mean Rewards 191.8\n",
      "Episode: 15600 Rewards: 916.0  Epsilon: 0.30  Mean Rewards 185.0\n",
      "Episode: 15700 Rewards: 83.0  Epsilon: 0.30  Mean Rewards 185.5\n",
      "Episode: 15800 Rewards: 22.0  Epsilon: 0.30  Mean Rewards 174.4\n",
      "Episode: 15900 Rewards: 323.0  Epsilon: 0.30  Mean Rewards 178.1\n",
      "Episode: 16000 Rewards: 240.0  Epsilon: 0.30  Mean Rewards 175.0\n",
      "Episode: 16100 Rewards: 259.0  Epsilon: 0.30  Mean Rewards 182.2\n",
      "Episode: 16200 Rewards: 140.0  Epsilon: 0.30  Mean Rewards 167.8\n",
      "Episode: 16300 Rewards: 197.0  Epsilon: 0.30  Mean Rewards 179.6\n",
      "Episode: 16400 Rewards: 199.0  Epsilon: 0.29  Mean Rewards 170.2\n",
      "Episode: 16500 Rewards: 105.0  Epsilon: 0.29  Mean Rewards 177.7\n",
      "Episode: 16600 Rewards: 100.0  Epsilon: 0.29  Mean Rewards 182.8\n",
      "Episode: 16700 Rewards: 97.0  Epsilon: 0.29  Mean Rewards 193.5\n",
      "Episode: 16800 Rewards: 210.0  Epsilon: 0.29  Mean Rewards 203.9\n",
      "Episode: 16900 Rewards: 119.0  Epsilon: 0.29  Mean Rewards 200.0\n",
      "Episode: 17000 Rewards: 278.0  Epsilon: 0.29  Mean Rewards 182.2\n",
      "Episode: 17100 Rewards: 185.0  Epsilon: 0.29  Mean Rewards 220.0\n",
      "Episode: 17200 Rewards: 282.0  Epsilon: 0.28  Mean Rewards 209.7\n",
      "Episode: 17300 Rewards: 250.0  Epsilon: 0.28  Mean Rewards 177.5\n",
      "Episode: 17400 Rewards: 220.0  Epsilon: 0.28  Mean Rewards 180.2\n",
      "Episode: 17500 Rewards: 196.0  Epsilon: 0.28  Mean Rewards 212.8\n",
      "Episode: 17600 Rewards: 47.0  Epsilon: 0.28  Mean Rewards 217.4\n",
      "Episode: 17700 Rewards: 157.0  Epsilon: 0.28  Mean Rewards 221.0\n",
      "Episode: 17800 Rewards: 160.0  Epsilon: 0.28  Mean Rewards 208.5\n",
      "Episode: 17900 Rewards: 195.0  Epsilon: 0.28  Mean Rewards 211.0\n",
      "Episode: 18000 Rewards: 79.0  Epsilon: 0.27  Mean Rewards 234.9\n",
      "Episode: 18100 Rewards: 211.0  Epsilon: 0.27  Mean Rewards 204.2\n",
      "Episode: 18200 Rewards: 262.0  Epsilon: 0.27  Mean Rewards 208.4\n",
      "Episode: 18300 Rewards: 228.0  Epsilon: 0.27  Mean Rewards 205.2\n",
      "Episode: 18400 Rewards: 240.0  Epsilon: 0.27  Mean Rewards 238.7\n",
      "Episode: 18500 Rewards: 208.0  Epsilon: 0.27  Mean Rewards 216.7\n",
      "Episode: 18600 Rewards: 276.0  Epsilon: 0.27  Mean Rewards 205.6\n",
      "Episode: 18700 Rewards: 402.0  Epsilon: 0.27  Mean Rewards 200.4\n",
      "Episode: 18800 Rewards: 468.0  Epsilon: 0.26  Mean Rewards 226.6\n",
      "Episode: 18900 Rewards: 464.0  Epsilon: 0.26  Mean Rewards 248.1\n",
      "Episode: 19000 Rewards: 169.0  Epsilon: 0.26  Mean Rewards 226.9\n",
      "Episode: 19100 Rewards: 207.0  Epsilon: 0.26  Mean Rewards 204.4\n",
      "Episode: 19200 Rewards: 130.0  Epsilon: 0.26  Mean Rewards 190.8\n",
      "Episode: 19300 Rewards: 198.0  Epsilon: 0.26  Mean Rewards 176.0\n",
      "Episode: 19400 Rewards: 212.0  Epsilon: 0.26  Mean Rewards 199.3\n",
      "Episode: 19500 Rewards: 112.0  Epsilon: 0.26  Mean Rewards 188.4\n",
      "Episode: 19600 Rewards: 201.0  Epsilon: 0.25  Mean Rewards 172.5\n",
      "Episode: 19700 Rewards: 190.0  Epsilon: 0.25  Mean Rewards 200.4\n",
      "Episode: 19800 Rewards: 223.0  Epsilon: 0.25  Mean Rewards 192.6\n",
      "Episode: 19900 Rewards: 421.0  Epsilon: 0.25  Mean Rewards 183.3\n",
      "Episode: 20000 Rewards: 198.0  Epsilon: 0.25  Mean Rewards 193.2\n",
      "Episode: 20100 Rewards: 232.0  Epsilon: 0.25  Mean Rewards 277.5\n",
      "Episode: 20200 Rewards: 210.0  Epsilon: 0.25  Mean Rewards 227.9\n",
      "Episode: 20300 Rewards: 188.0  Epsilon: 0.25  Mean Rewards 182.5\n",
      "Episode: 20400 Rewards: 217.0  Epsilon: 0.24  Mean Rewards 216.2\n",
      "Episode: 20500 Rewards: 381.0  Epsilon: 0.24  Mean Rewards 229.5\n",
      "Episode: 20600 Rewards: 171.0  Epsilon: 0.24  Mean Rewards 227.8\n",
      "Episode: 20700 Rewards: 273.0  Epsilon: 0.24  Mean Rewards 238.9\n",
      "Episode: 20800 Rewards: 300.0  Epsilon: 0.24  Mean Rewards 206.9\n",
      "Episode: 20900 Rewards: 245.0  Epsilon: 0.24  Mean Rewards 274.5\n",
      "Episode: 21000 Rewards: 280.0  Epsilon: 0.24  Mean Rewards 286.7\n",
      "Episode: 21100 Rewards: 233.0  Epsilon: 0.24  Mean Rewards 248.4\n",
      "Episode: 21200 Rewards: 508.0  Epsilon: 0.23  Mean Rewards 281.8\n",
      "Episode: 21300 Rewards: 34.0  Epsilon: 0.23  Mean Rewards 248.6\n",
      "Episode: 21400 Rewards: 384.0  Epsilon: 0.23  Mean Rewards 248.4\n",
      "Episode: 21500 Rewards: 287.0  Epsilon: 0.23  Mean Rewards 258.5\n",
      "Episode: 21600 Rewards: 298.0  Epsilon: 0.23  Mean Rewards 263.6\n",
      "Episode: 21700 Rewards: 66.0  Epsilon: 0.23  Mean Rewards 222.4\n",
      "Episode: 21800 Rewards: 210.0  Epsilon: 0.23  Mean Rewards 219.5\n",
      "Episode: 21900 Rewards: 345.0  Epsilon: 0.23  Mean Rewards 227.5\n",
      "Episode: 22000 Rewards: 153.0  Epsilon: 0.22  Mean Rewards 202.9\n",
      "Episode: 22100 Rewards: 210.0  Epsilon: 0.22  Mean Rewards 244.8\n",
      "Episode: 22200 Rewards: 482.0  Epsilon: 0.22  Mean Rewards 233.4\n",
      "Episode: 22300 Rewards: 352.0  Epsilon: 0.22  Mean Rewards 293.1\n",
      "Episode: 22400 Rewards: 351.0  Epsilon: 0.22  Mean Rewards 304.7\n",
      "Episode: 22500 Rewards: 415.0  Epsilon: 0.22  Mean Rewards 251.3\n",
      "Episode: 22600 Rewards: 213.0  Epsilon: 0.22  Mean Rewards 278.4\n",
      "Episode: 22700 Rewards: 387.0  Epsilon: 0.22  Mean Rewards 266.2\n",
      "Episode: 22800 Rewards: 263.0  Epsilon: 0.21  Mean Rewards 267.6\n",
      "Episode: 22900 Rewards: 256.0  Epsilon: 0.21  Mean Rewards 207.1\n",
      "Episode: 23000 Rewards: 198.0  Epsilon: 0.21  Mean Rewards 239.4\n",
      "Episode: 23100 Rewards: 247.0  Epsilon: 0.21  Mean Rewards 222.3\n",
      "Episode: 23200 Rewards: 122.0  Epsilon: 0.21  Mean Rewards 229.9\n",
      "Episode: 23300 Rewards: 114.0  Epsilon: 0.21  Mean Rewards 242.3\n",
      "Episode: 23400 Rewards: 182.0  Epsilon: 0.21  Mean Rewards 238.1\n",
      "Episode: 23500 Rewards: 216.0  Epsilon: 0.21  Mean Rewards 285.6\n",
      "Episode: 23600 Rewards: 106.0  Epsilon: 0.20  Mean Rewards 324.3\n",
      "Episode: 23700 Rewards: 215.0  Epsilon: 0.20  Mean Rewards 210.6\n",
      "Episode: 23800 Rewards: 298.0  Epsilon: 0.20  Mean Rewards 239.1\n",
      "Episode: 23900 Rewards: 227.0  Epsilon: 0.20  Mean Rewards 316.2\n",
      "Episode: 24000 Rewards: 406.0  Epsilon: 0.20  Mean Rewards 280.9\n",
      "Episode: 24100 Rewards: 421.0  Epsilon: 0.20  Mean Rewards 317.6\n",
      "Episode: 24200 Rewards: 133.0  Epsilon: 0.20  Mean Rewards 325.0\n",
      "Episode: 24300 Rewards: 188.0  Epsilon: 0.20  Mean Rewards 314.8\n",
      "Episode: 24400 Rewards: 246.0  Epsilon: 0.19  Mean Rewards 270.7\n",
      "Episode: 24500 Rewards: 68.0  Epsilon: 0.19  Mean Rewards 330.0\n",
      "Episode: 24600 Rewards: 152.0  Epsilon: 0.19  Mean Rewards 239.9\n",
      "Episode: 24700 Rewards: 344.0  Epsilon: 0.19  Mean Rewards 314.9\n",
      "Episode: 24800 Rewards: 196.0  Epsilon: 0.19  Mean Rewards 273.5\n",
      "Episode: 24900 Rewards: 332.0  Epsilon: 0.19  Mean Rewards 243.7\n",
      "Episode: 25000 Rewards: 245.0  Epsilon: 0.19  Mean Rewards 265.1\n",
      "Episode: 25100 Rewards: 405.0  Epsilon: 0.19  Mean Rewards 257.6\n",
      "Episode: 25200 Rewards: 432.0  Epsilon: 0.18  Mean Rewards 329.7\n",
      "Episode: 25300 Rewards: 288.0  Epsilon: 0.18  Mean Rewards 272.1\n",
      "Episode: 25400 Rewards: 140.0  Epsilon: 0.18  Mean Rewards 256.1\n",
      "Episode: 25500 Rewards: 357.0  Epsilon: 0.18  Mean Rewards 348.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 25600 Rewards: 544.0  Epsilon: 0.18  Mean Rewards 290.9\n",
      "Episode: 25700 Rewards: 488.0  Epsilon: 0.18  Mean Rewards 399.9\n",
      "Episode: 25800 Rewards: 209.0  Epsilon: 0.18  Mean Rewards 322.2\n",
      "Episode: 25900 Rewards: 214.0  Epsilon: 0.18  Mean Rewards 304.3\n",
      "Episode: 26000 Rewards: 254.0  Epsilon: 0.17  Mean Rewards 270.7\n",
      "Episode: 26100 Rewards: 189.0  Epsilon: 0.17  Mean Rewards 312.4\n",
      "Episode: 26200 Rewards: 265.0  Epsilon: 0.17  Mean Rewards 279.3\n",
      "Episode: 26300 Rewards: 290.0  Epsilon: 0.17  Mean Rewards 296.1\n",
      "Episode: 26400 Rewards: 227.0  Epsilon: 0.17  Mean Rewards 300.4\n",
      "Episode: 26500 Rewards: 222.0  Epsilon: 0.17  Mean Rewards 296.1\n",
      "Episode: 26600 Rewards: 299.0  Epsilon: 0.17  Mean Rewards 263.8\n",
      "Episode: 26700 Rewards: 281.0  Epsilon: 0.17  Mean Rewards 270.0\n",
      "Episode: 26800 Rewards: 385.0  Epsilon: 0.16  Mean Rewards 283.8\n",
      "Episode: 26900 Rewards: 80.0  Epsilon: 0.16  Mean Rewards 294.9\n",
      "Episode: 27000 Rewards: 152.0  Epsilon: 0.16  Mean Rewards 213.7\n",
      "Episode: 27100 Rewards: 202.0  Epsilon: 0.16  Mean Rewards 195.5\n",
      "Episode: 27200 Rewards: 265.0  Epsilon: 0.16  Mean Rewards 231.1\n",
      "Episode: 27300 Rewards: 239.0  Epsilon: 0.16  Mean Rewards 303.9\n",
      "Episode: 27400 Rewards: 181.0  Epsilon: 0.16  Mean Rewards 224.3\n",
      "Episode: 27500 Rewards: 213.0  Epsilon: 0.16  Mean Rewards 252.8\n",
      "Episode: 27600 Rewards: 189.0  Epsilon: 0.15  Mean Rewards 260.3\n",
      "Episode: 27700 Rewards: 211.0  Epsilon: 0.15  Mean Rewards 223.1\n",
      "Episode: 27800 Rewards: 218.0  Epsilon: 0.15  Mean Rewards 231.6\n",
      "Episode: 27900 Rewards: 171.0  Epsilon: 0.15  Mean Rewards 228.6\n",
      "Episode: 28000 Rewards: 175.0  Epsilon: 0.15  Mean Rewards 200.7\n",
      "Episode: 28100 Rewards: 201.0  Epsilon: 0.15  Mean Rewards 207.8\n",
      "Episode: 28200 Rewards: 705.0  Epsilon: 0.15  Mean Rewards 273.4\n",
      "Episode: 28300 Rewards: 251.0  Epsilon: 0.15  Mean Rewards 304.9\n",
      "Episode: 28400 Rewards: 168.0  Epsilon: 0.14  Mean Rewards 259.2\n",
      "Episode: 28500 Rewards: 211.0  Epsilon: 0.14  Mean Rewards 222.4\n",
      "Episode: 28600 Rewards: 269.0  Epsilon: 0.14  Mean Rewards 241.7\n",
      "Episode: 28700 Rewards: 231.0  Epsilon: 0.14  Mean Rewards 257.3\n",
      "Episode: 28800 Rewards: 171.0  Epsilon: 0.14  Mean Rewards 261.8\n",
      "Episode: 28900 Rewards: 238.0  Epsilon: 0.14  Mean Rewards 280.9\n",
      "Episode: 29000 Rewards: 445.0  Epsilon: 0.14  Mean Rewards 374.8\n",
      "Episode: 29100 Rewards: 577.0  Epsilon: 0.14  Mean Rewards 296.3\n",
      "Episode: 29200 Rewards: 265.0  Epsilon: 0.13  Mean Rewards 400.2\n",
      "Episode: 29300 Rewards: 465.0  Epsilon: 0.13  Mean Rewards 313.6\n",
      "Episode: 29400 Rewards: 163.0  Epsilon: 0.13  Mean Rewards 263.1\n",
      "Episode: 29500 Rewards: 507.0  Epsilon: 0.13  Mean Rewards 321.2\n",
      "Episode: 29600 Rewards: 202.0  Epsilon: 0.13  Mean Rewards 253.5\n",
      "Episode: 29700 Rewards: 349.0  Epsilon: 0.13  Mean Rewards 257.5\n",
      "Episode: 29800 Rewards: 166.0  Epsilon: 0.13  Mean Rewards 268.2\n",
      "Episode: 29900 Rewards: 209.0  Epsilon: 0.13  Mean Rewards 298.3\n",
      "Episode: 30000 Rewards: 154.0  Epsilon: 0.12  Mean Rewards 262.7\n",
      "Episode: 30100 Rewards: 235.0  Epsilon: 0.12  Mean Rewards 296.5\n",
      "Episode: 30200 Rewards: 165.0  Epsilon: 0.12  Mean Rewards 270.6\n",
      "Episode: 30300 Rewards: 214.0  Epsilon: 0.12  Mean Rewards 252.7\n",
      "Episode: 30400 Rewards: 189.0  Epsilon: 0.12  Mean Rewards 318.7\n",
      "Episode: 30500 Rewards: 233.0  Epsilon: 0.12  Mean Rewards 312.3\n",
      "Episode: 30600 Rewards: 741.0  Epsilon: 0.12  Mean Rewards 336.0\n",
      "Episode: 30700 Rewards: 207.0  Epsilon: 0.12  Mean Rewards 278.6\n",
      "Episode: 30800 Rewards: 232.0  Epsilon: 0.11  Mean Rewards 209.0\n",
      "Episode: 30900 Rewards: 213.0  Epsilon: 0.11  Mean Rewards 233.1\n",
      "Episode: 31000 Rewards: 226.0  Epsilon: 0.11  Mean Rewards 239.7\n",
      "Episode: 31100 Rewards: 486.0  Epsilon: 0.11  Mean Rewards 345.1\n",
      "Episode: 31200 Rewards: 171.0  Epsilon: 0.11  Mean Rewards 277.6\n",
      "Episode: 31300 Rewards: 207.0  Epsilon: 0.11  Mean Rewards 273.6\n",
      "Episode: 31400 Rewards: 414.0  Epsilon: 0.11  Mean Rewards 348.3\n",
      "Episode: 31500 Rewards: 231.0  Epsilon: 0.11  Mean Rewards 258.3\n",
      "Episode: 31600 Rewards: 420.0  Epsilon: 0.10  Mean Rewards 314.4\n",
      "Episode: 31700 Rewards: 237.0  Epsilon: 0.10  Mean Rewards 377.2\n",
      "Episode: 31800 Rewards: 541.0  Epsilon: 0.10  Mean Rewards 393.3\n",
      "Episode: 31900 Rewards: 235.0  Epsilon: 0.10  Mean Rewards 284.7\n",
      "Episode: 32000 Rewards: 296.0  Epsilon: 0.10  Mean Rewards 269.0\n",
      "Episode: 32100 Rewards: 239.0  Epsilon: 0.10  Mean Rewards 293.1\n",
      "Episode: 32200 Rewards: 614.0  Epsilon: 0.10  Mean Rewards 237.0\n",
      "Episode: 32300 Rewards: 250.0  Epsilon: 0.10  Mean Rewards 350.5\n",
      "Episode: 32400 Rewards: 282.0  Epsilon: 0.09  Mean Rewards 302.1\n",
      "Episode: 32500 Rewards: 256.0  Epsilon: 0.09  Mean Rewards 286.8\n",
      "Episode: 32600 Rewards: 304.0  Epsilon: 0.09  Mean Rewards 281.6\n",
      "Episode: 32700 Rewards: 268.0  Epsilon: 0.09  Mean Rewards 431.1\n",
      "Episode: 32800 Rewards: 273.0  Epsilon: 0.09  Mean Rewards 281.7\n",
      "Episode: 32900 Rewards: 456.0  Epsilon: 0.09  Mean Rewards 270.2\n",
      "Episode: 33000 Rewards: 298.0  Epsilon: 0.09  Mean Rewards 243.3\n",
      "Episode: 33100 Rewards: 201.0  Epsilon: 0.09  Mean Rewards 225.2\n",
      "Episode: 33200 Rewards: 256.0  Epsilon: 0.08  Mean Rewards 246.2\n",
      "Episode: 33300 Rewards: 216.0  Epsilon: 0.08  Mean Rewards 245.9\n",
      "Episode: 33400 Rewards: 236.0  Epsilon: 0.08  Mean Rewards 228.8\n",
      "Episode: 33500 Rewards: 225.0  Epsilon: 0.08  Mean Rewards 221.6\n",
      "Episode: 33600 Rewards: 212.0  Epsilon: 0.08  Mean Rewards 226.9\n",
      "Episode: 33700 Rewards: 284.0  Epsilon: 0.08  Mean Rewards 292.4\n",
      "Episode: 33800 Rewards: 212.0  Epsilon: 0.08  Mean Rewards 248.1\n",
      "Episode: 33900 Rewards: 195.0  Epsilon: 0.08  Mean Rewards 304.6\n",
      "Episode: 34000 Rewards: 287.0  Epsilon: 0.07  Mean Rewards 272.0\n",
      "Episode: 34100 Rewards: 166.0  Epsilon: 0.07  Mean Rewards 244.9\n",
      "Episode: 34200 Rewards: 222.0  Epsilon: 0.07  Mean Rewards 282.6\n",
      "Episode: 34300 Rewards: 261.0  Epsilon: 0.07  Mean Rewards 262.6\n",
      "Episode: 34400 Rewards: 357.0  Epsilon: 0.07  Mean Rewards 257.3\n",
      "Episode: 34500 Rewards: 279.0  Epsilon: 0.07  Mean Rewards 264.9\n",
      "Episode: 34600 Rewards: 227.0  Epsilon: 0.07  Mean Rewards 266.7\n",
      "Episode: 34700 Rewards: 341.0  Epsilon: 0.07  Mean Rewards 241.1\n",
      "Episode: 34800 Rewards: 223.0  Epsilon: 0.06  Mean Rewards 235.0\n",
      "Episode: 34900 Rewards: 191.0  Epsilon: 0.06  Mean Rewards 252.4\n",
      "Episode: 35000 Rewards: 259.0  Epsilon: 0.06  Mean Rewards 292.9\n",
      "Episode: 35100 Rewards: 230.0  Epsilon: 0.06  Mean Rewards 289.3\n",
      "Episode: 35200 Rewards: 258.0  Epsilon: 0.06  Mean Rewards 236.7\n",
      "Episode: 35300 Rewards: 203.0  Epsilon: 0.06  Mean Rewards 259.9\n",
      "Episode: 35400 Rewards: 253.0  Epsilon: 0.06  Mean Rewards 254.8\n",
      "Episode: 35500 Rewards: 269.0  Epsilon: 0.06  Mean Rewards 314.0\n",
      "Episode: 35600 Rewards: 221.0  Epsilon: 0.05  Mean Rewards 279.5\n",
      "Episode: 35700 Rewards: 241.0  Epsilon: 0.05  Mean Rewards 319.4\n",
      "Episode: 35800 Rewards: 205.0  Epsilon: 0.05  Mean Rewards 346.3\n",
      "Episode: 35900 Rewards: 204.0  Epsilon: 0.05  Mean Rewards 233.4\n",
      "Episode: 36000 Rewards: 245.0  Epsilon: 0.05  Mean Rewards 228.6\n",
      "Episode: 36100 Rewards: 257.0  Epsilon: 0.05  Mean Rewards 232.7\n",
      "Episode: 36200 Rewards: 233.0  Epsilon: 0.05  Mean Rewards 248.8\n",
      "Episode: 36300 Rewards: 235.0  Epsilon: 0.05  Mean Rewards 261.2\n",
      "Episode: 36400 Rewards: 190.0  Epsilon: 0.04  Mean Rewards 238.4\n",
      "Episode: 36500 Rewards: 262.0  Epsilon: 0.04  Mean Rewards 270.4\n",
      "Episode: 36600 Rewards: 4360.0  Epsilon: 0.04  Mean Rewards 379.4\n",
      "Episode: 36700 Rewards: 215.0  Epsilon: 0.04  Mean Rewards 281.4\n",
      "Episode: 36800 Rewards: 227.0  Epsilon: 0.04  Mean Rewards 238.9\n",
      "Episode: 36900 Rewards: 260.0  Epsilon: 0.04  Mean Rewards 226.4\n",
      "Episode: 37000 Rewards: 524.0  Epsilon: 0.04  Mean Rewards 247.1\n",
      "Episode: 37100 Rewards: 220.0  Epsilon: 0.04  Mean Rewards 253.1\n",
      "Episode: 37200 Rewards: 642.0  Epsilon: 0.03  Mean Rewards 894.5\n",
      "Episode: 37300 Rewards: 246.0  Epsilon: 0.03  Mean Rewards 403.7\n",
      "Episode: 37400 Rewards: 213.0  Epsilon: 0.03  Mean Rewards 375.9\n",
      "Episode: 37500 Rewards: 204.0  Epsilon: 0.03  Mean Rewards 287.4\n",
      "Episode: 37600 Rewards: 228.0  Epsilon: 0.03  Mean Rewards 231.0\n",
      "Episode: 37700 Rewards: 519.0  Epsilon: 0.03  Mean Rewards 484.8\n",
      "Episode: 37800 Rewards: 213.0  Epsilon: 0.03  Mean Rewards 587.1\n",
      "Episode: 37900 Rewards: 263.0  Epsilon: 0.03  Mean Rewards 248.4\n",
      "Episode: 38000 Rewards: 269.0  Epsilon: 0.02  Mean Rewards 283.2\n",
      "Episode: 38100 Rewards: 238.0  Epsilon: 0.02  Mean Rewards 484.8\n",
      "Episode: 38200 Rewards: 273.0  Epsilon: 0.02  Mean Rewards 316.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 38300 Rewards: 259.0  Epsilon: 0.02  Mean Rewards 405.4\n",
      "Episode: 38400 Rewards: 236.0  Epsilon: 0.02  Mean Rewards 265.8\n",
      "Episode: 38500 Rewards: 449.0  Epsilon: 0.02  Mean Rewards 318.1\n",
      "Episode: 38600 Rewards: 838.0  Epsilon: 0.02  Mean Rewards 470.4\n",
      "Episode: 38700 Rewards: 329.0  Epsilon: 0.02  Mean Rewards 519.4\n",
      "Episode: 38800 Rewards: 291.0  Epsilon: 0.01  Mean Rewards 558.9\n",
      "Episode: 38900 Rewards: 223.0  Epsilon: 0.01  Mean Rewards 309.4\n",
      "Episode: 39000 Rewards: 416.0  Epsilon: 0.01  Mean Rewards 371.1\n",
      "Episode: 39100 Rewards: 279.0  Epsilon: 0.01  Mean Rewards 299.1\n",
      "Episode: 39200 Rewards: 387.0  Epsilon: 0.01  Mean Rewards 306.4\n",
      "Episode: 39300 Rewards: 243.0  Epsilon: 0.01  Mean Rewards 293.8\n",
      "Episode: 39400 Rewards: 275.0  Epsilon: 0.01  Mean Rewards 305.8\n",
      "Episode: 39500 Rewards: 413.0  Epsilon: 0.01  Mean Rewards 389.2\n",
      "Episode: 39600 Rewards: 444.0  Epsilon: 0.00  Mean Rewards 411.9\n",
      "Episode: 39700 Rewards: 412.0  Epsilon: 0.00  Mean Rewards 402.5\n",
      "Episode: 39800 Rewards: 373.0  Epsilon: 0.00  Mean Rewards 406.0\n",
      "Episode: 39900 Rewards: 434.0  Epsilon: 0.00  Mean Rewards 418.2\n",
      "Average reward after all episodes:  206.0928712871287\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    gamma = 0.7 # Discount rate\n",
    "    alpha = 0.1 # Learning rate\n",
    "    epsilon = 0.5 # How much we want to explore \n",
    "    episodes = 40_000 # Number of episodes\n",
    "\n",
    "    isLearning = True # Set to False to test the trained model\n",
    "\n",
    "    cart_pole = CartPole(isLearning)\n",
    "    agent = Q_learning(cart_pole, gamma, alpha, epsilon, episodes, isLearning)\n",
    "    agent.apply()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f66ffc-410d-43a9-87d8-d36aa407ec35",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copy of main to visualize\n",
    "def main():\n",
    "    gamma = 0.7 # Discount rate\n",
    "    alpha = 0.1 # Learning rate\n",
    "    epsilon = 0.5 # How much we want to explore \n",
    "    episodes = 40_000 # Number of episodes\n",
    "\n",
    "    isLearning = False # Set to False to test the trained model\n",
    "\n",
    "    cart_pole = CartPole(isLearning)\n",
    "    agent = Q_learning(cart_pole, gamma, alpha, epsilon, episodes, isLearning)\n",
    "    agent.apply()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c17dc-4aeb-44d7-9f9b-f16f3b459bf8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
