{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6a8e7c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 16 - Cart Pole Q-learning\n",
    "### Author: Michał Krępa 6475383\n",
    "\n",
    "This project is a solution to the assignment given during AI For Robotics I course at UniGe.\n",
    "The project number is 16.\n",
    "\n",
    "## Project Structure\n",
    "The project consists of two 3 components:\n",
    "- `CartPole`: A wrapper class for OpenAI Gymnasium CartPole environment\n",
    "- `Q_learnign`: Agent which implements Qlearnig algorithm\n",
    "\n",
    "- `main.py`: Main function with all the variables\n",
    "\n",
    "## Requirements\n",
    "To successfully run the project few dependencies are needed.\n",
    "\n",
    "- `gymnasium` - Library from OpenAI containing API with required Environments\n",
    "- `numpy` -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4dc2e3a-01e3-44e8-b662-6b9874143ec6",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (from gymnasium) (1.24.3)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (from gymnasium) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: numpy in /Users/michalkrepa/anaconda3/lib/python3.11/site-packages (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d46979-dd94-47ed-8eaf-9bcfbe306b6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b39881-4fb6-460b-9261-513432e40921",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CartPole class\n",
    "This is a wrapper class for  Gymnasium's CartPole environmnent. The reason I had in mind while creating this, was to follow the example given during lab sessions. While creating it I spotted the main challenge I had with this project - The Environment generates a continuous observation space, which later on has to be turned discrete, if we want to have some efficient computations about it. After some struggle, I ended up with the observation space digitized into 10 pieces.\n",
    "\n",
    "Additionally this wrapper function has some member functions that are meant to return specific values, more details in the code.\n",
    "\n",
    "The object of this class can be initialized using the flag of `is_learning` set to `False` to get the visualizations of the CartPole environment. Nevertheless, this is not recommended while training the model, as it may prolong the training time effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6d1643-dc6c-475e-a3cd-b2c48f12c98b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CartPole:\n",
    "    \"\"\"\n",
    "        Wrapper class for CartPole environment\n",
    "\n",
    "        Attributes:\n",
    "            _env: The Gym environment for the Cart Pole game.\n",
    "            _curr_state (np.array): The current state of the environment.\n",
    "            _isTerminated (bool): Flag indicating whether the current episode has ended.\n",
    "    \"\"\"\n",
    "    def __init__(self, is_learning = False):\n",
    "        \"\"\"\n",
    "        Initializes the CartPole environment\n",
    "\n",
    "        Args:\n",
    "            is_learning (bool): Flag to determine if the environment is for learning or visualization.\n",
    "        \"\"\"\n",
    "        # Define whether we want to visualize\n",
    "        if is_learning:\n",
    "            self._env = gym.make('CartPole-v1')\n",
    "        else:\n",
    "            self._env = gym.make('CartPole-v1', render_mode = \"human\")\n",
    "        self._currState = self._env.reset()[0]\n",
    "        self._isTerminated = False\n",
    "\n",
    "\n",
    "    def digitize_state(self, state):\n",
    "        \"\"\"\n",
    "        Digitizes the continuous state into discrete values for Q-table.\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            list: A list representing the digitized state.\n",
    "        \"\"\"\n",
    "        pos_space = np.linspace(-2.4, 2.4, 10)\n",
    "        vel_space = np.linspace(-4, 4, 10)\n",
    "        ang_space = np.linspace(-.2095, .2095, 10)\n",
    "        ang_vel_space = np.linspace(-4, 4, 10)\n",
    "        \n",
    "        new_state_p = np.digitize(state[0], pos_space)\n",
    "        new_state_v = np.digitize(state[1], vel_space)\n",
    "        new_state_a = np.digitize(state[2], ang_space)\n",
    "        new_state_av= np.digitize(state[3], ang_vel_space)\n",
    "        new_state_dig = [new_state_p, new_state_v, new_state_a, new_state_av]\n",
    "        return new_state_dig\n",
    "\n",
    "    def do_action(self, action):\n",
    "       \"\"\"\n",
    "        Performs a step in the environment. Gets the values for Observation, reward and checks if the game is over\n",
    "\n",
    "        Args:\n",
    "            action (int): an action passed to the environment\n",
    "        Returns:\n",
    "            new_state: Discrete state after the action is taken\n",
    "            reward: Reward basing on the taken action\n",
    "       \"\"\"\n",
    "       new_state, reward, self._isTerminated, _, _ = self._env.step(action)       \n",
    "       # Update the current state\n",
    "       self._currState = new_state\n",
    "       return self.digitize_state(new_state), reward\n",
    "    \n",
    "    def reset_env(self):\n",
    "        \"\"\" Resets the environment \"\"\"\n",
    "        self._currState = self._env.reset()[0]\n",
    "        self._isTerminated = False\n",
    "\n",
    "    def get_current_state(self):\n",
    "        \"\"\" Gets the discrete state of the environment \"\"\"\n",
    "        return self.digitize_state(self._currState)\n",
    "    \n",
    "    def get_action_space(self):\n",
    "        \"\"\"Returns the size of the action space\"\"\"\n",
    "        return self._env.action_space.n\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        \"\"\" Returns boolean determining if game is over\"\"\"\n",
    "        return self._isTerminated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae1321-8bd5-435b-9b05-74fd1cff6693",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Q_learning Agent\n",
    "The class that contains the Q_learning agent follows, or at least attempts to follow the style shown during lab sessions. Inside this class there are few interesting things:\n",
    "\n",
    "- `_Q_table` - the object of Q-table is not a dictionary of dictionaries as it was done during the class, as I found it difficult to do that for a 4 dimensional state space (or observation space). I am also not that good with python, so basically tried to create a 4-dim array of lists + action space (which in our case is pretty simple. The cart can only go left or right, so two values). This object ends up being initialized as `11x11x11x11x2` array, which in my opinion is quite big, but thought of it as necessary to get rather precise results in the longer run.\n",
    "\n",
    "\n",
    "- **Policy** - For the policy I am using the *Epsilon Greedy Policy* with an additional modifications. I wanted to train my model, so added there a boolean for learning, to distinguish if we want to explore or just use the values from the Q-table\n",
    "\n",
    "- Epsilon Decay Rate - While doing my own research for this project I noticed that Epsilon Greedy Policy can be adjusted with additional use of the Epsilon that is decaying over the episodes, having first fully random choice, and then turning the use to rely more and more on the Q-table. I was unsure whether I could use this or not, but In the end it can be found in the code with a proper comments next to it. If the user would like to turn it of, they can simply comment that one line or remove it from the code\n",
    "\n",
    "```python\n",
    "            # Epsilon Decay rate \n",
    "            self._epsilon = max(self._epsilon - self._epsilon/sum(episodes), 0)\n",
    "```\n",
    "\n",
    "- `apply` - this function does the whole simulation. It works in two modes. Depending on `isLearning` flag, the first mode is meant for learning, second one is meant to use the Q-table to \"show off\" the capabilities of trained model.\n",
    "if we run the agent with `isLearning` set to `True` at the end we will receive an object of `Q_table.pkl `. This is the filled Q-table that will be loaded when `isLearning` will be set to `False`.  Finally this function also provides some data for statistics to see whether our model is learning efficiently or not. Every 100 episodes it prints out the message as in the example below:\n",
    "```\n",
    "Episode: 81500 Rewards: 142.0  Epsilon: 0.44  Mean Rewards 80.8\n",
    "```\n",
    "Where Episode - is the current episode number, Rewards go for the obtained rewards for that current episode, Epsilon shows the current value of the Epsilone and Mean rewards showing the average score for 100 episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebece168-ad20-47c6-8aa9-7ffe84d76855",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Q_learning:\n",
    "    \"\"\"\n",
    "        Implementation of Q-learning algorhitm for the CartPole environment.\n",
    "\n",
    "        Attributes:\n",
    "            _env (cartPoleEnv): Cart Pole env\n",
    "            _gamma (float):   The discount factor\n",
    "            _alpha (float): The learning rate.\n",
    "            _epsilon (float): The exploration rate.\n",
    "            _episodes (int): The number of episodes for training\n",
    "            _is_learning (bool): Flag indicating whether the agent is in learning mode.\n",
    "            _Q_table (np.array): The Q-table, stores state-action values\n",
    "    \"\"\"\n",
    "    def __init__(self, env, gamma, alpha, epsilon, episodes, isLearning = True):\n",
    "        \"\"\"\n",
    "            Initializes Q-learning agent.\n",
    "\n",
    "            Works in two ways. When isLearning flag is set True,\n",
    "            it initializes Q-table as a empty np.array, else it tries to load it from file.\n",
    "            Args:\n",
    "                env (cartPoleEnv): The Cart Pole environment.\n",
    "                gamma (float): The discount factor.\n",
    "                alpha (float): The learning rate.\n",
    "                epsilon (float): The exploration rate.\n",
    "                episodes (int): The number of episodes for training.\n",
    "                isLearning (bool): Flag to determine if the agent is in learning mode.\n",
    "        \"\"\"\n",
    "        self._env = env\n",
    "        self._gamma = gamma\n",
    "        self._alpha = alpha\n",
    "        self._epsilon = epsilon\n",
    "        self._episodes = episodes\n",
    "        self._isLearning = isLearning\n",
    "\n",
    "        # Initialize Q_Table\n",
    "        if self._isLearning: \n",
    "            # State is given as continuous set of variables\n",
    "            # we need to cut it into pieces to be able to learn\n",
    "            # The limits here are the limits for our game to be over\n",
    "            pos_space = np.linspace(-2.4, 2.4, 10)\n",
    "            vel_space = np.linspace(-4, 4, 10)\n",
    "            ang_space = np.linspace(-.2095, .2095, 10) #value in rad\n",
    "            ang_vel_space = np.linspace(-4, 4, 10)\n",
    "            self.Q_table = np.zeros((len(pos_space)+1, len(vel_space)+1, \n",
    "                                    len(ang_space)+1, len(ang_vel_space)+1, self._env.get_action_space())) #11x11x11x11x2\n",
    "        else:\n",
    "            #Load the model\n",
    "            f = open('Q_table.pkl', 'rb')\n",
    "            self.Q_table = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\" \n",
    "        Epsilon Greedy Policy\n",
    "\n",
    "        Function works in two modes:\n",
    "            If isLearning is True, decides on random whether to choose random action or\n",
    "            the best action according to the Q_table. The higher epsilon, the higher chance of getting random results\n",
    "            When isLearning is set to False, policy only chooses the values basing on the Q_table.\n",
    "        \n",
    "        Args:\n",
    "            state: Discrete state of the environment\n",
    "\n",
    "        \"\"\"\n",
    "        if self._isLearning and np.random.random() < self._epsilon:\n",
    "            # Choose an action at random with probability epsilon\n",
    "            return random.choice([0,1]) # only two actions - left or right\n",
    "        else:\n",
    "            # Choose the best action accordin to Q_table with probability 1-epsilon\n",
    "            return np.argmax(self.Q_table[state[0], state[1], state[2], state[3], :])\n",
    "\n",
    "    def apply(self):\n",
    "        \"\"\"\n",
    "        Executes Q-learning algorhithm over a specified number of episodes.\n",
    "\n",
    "        This method runs the Q-learning algorithm, updating the Q-table based on the interactions\n",
    "        with the environment. It implements an epsilon-greedy policy for action selection and applies \n",
    "        temporal difference learning for updating the Q-table.\n",
    "        Additionally, the method also handles epsilon decay.\n",
    "         \n",
    "        For exploration over time and prints out the progress every 100 episodes.\n",
    "\n",
    "        The method performs the following steps in each episode:\n",
    "        - Interacts with the environment to obtain states, rewards, and new states.\n",
    "        - Updates the Q-table using the temporal difference\n",
    "        - Applies epsilon decay to gradually shift from exploration to exploitation.\n",
    "        - Tracks and logs the rewards for each episode.\n",
    "\n",
    "        At the end of the training, the updated Q-table is saved to a file (if in learning mode), \n",
    "        and the average reward across all episodes is calculated and printed to the output.\n",
    "        \"\"\"\n",
    "\n",
    "        total_episode_rewards = []  # Rewards of all runs\n",
    "        for episode in range(self._episodes):\n",
    "            episode_rewards = [] # rewards for each episode\n",
    "            rewards = 0\n",
    "            while not self._env.is_game_over():\n",
    "                # get the current state\n",
    "                curr_state = self._env.get_current_state()\n",
    "                action = self.policy(curr_state)\n",
    "                next_state, reward = self._env.do_action(action)\n",
    "                # Choose maximum Q-value for next state\n",
    "                max_next_value = np.max(self.Q_table[next_state[0], next_state[1], next_state[2], next_state[3], :])\n",
    "                # Temporal difference update TODO improve readability\n",
    "                self.Q_table[curr_state[0], curr_state[1], curr_state[2], curr_state[3], action] = self.Q_table[curr_state[0], curr_state[1], curr_state[2], curr_state[3], action] +\\\n",
    "                self._alpha * ( reward + self._gamma * max_next_value -  self.Q_table[curr_state[0], curr_state[1], curr_state[2], curr_state[3], action]) \n",
    "                rewards += reward\n",
    "\n",
    "            # Reset before new episode\n",
    "            self._env.reset_env()\n",
    "\n",
    "            # Epsilon Decay rate \n",
    "            self._epsilon = max(self._epsilon - self._epsilon/self._episodes, 0)\n",
    "            \n",
    "            # Get episode  rewards\n",
    "            total_episode_rewards.append(rewards)\n",
    "            mean_rewards = np.mean(total_episode_rewards[len(total_episode_rewards)-100:])\n",
    "            # For every 100 display rewards\n",
    "            if episode % 1000 == 0:\n",
    "                print(f'Episode: {episode} Rewards: {rewards}  Epsilon: {self._epsilon:0.2f}  Mean Rewards {mean_rewards:0.1f}')\n",
    "            total_episode_rewards.append(np.sum(episode_rewards))\n",
    "        \n",
    "        # Save Q table to file\n",
    "        if self._isLearning:\n",
    "            f = open('Q_table.pkl','wb')\n",
    "            pickle.dump(self.Q_table, f)\n",
    "            f.close()\n",
    "\n",
    "        # Calculate the mean\n",
    "        print(\"Average reward after all episodes: \", np.mean(total_episode_rewards))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3681a5-4be5-4bf5-af4a-408f7a30047d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The main function\n",
    "Finally, the main function defines the constant variables as `gamma` `alpha` `epsilon` or the amount of `episodes`.\n",
    "For this project I had chosen a big amount of episodes, justifying it by the size of the Q-table. A huge object like that will require many steps and episodes to be eventually filled and therefore bigger amount of steps.\n",
    "\n",
    "If I were to decide, this is probably not the best setup for this environment and Q-learning, as I was playing with different values I could get various results, sometimes having more than 100 000 episodes lead me to obtaining even +1k rewards for an episode, but in the end this works fine, after around 20 k we see the increase of average rewards and this continues till the end of simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92b21e15-8874-4619-9e61-7061e18c77c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Rewards: 262.0  Epsilon: 1.00  Mean Rewards 262.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     agent\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 14\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m cart_pole \u001b[38;5;241m=\u001b[39m CartPole(isLearning)\n\u001b[1;32m     10\u001b[0m agent \u001b[38;5;241m=\u001b[39m Q_learning(cart_pole, gamma, alpha, epsilon, episodes, isLearning)\n\u001b[0;32m---> 11\u001b[0m agent\u001b[38;5;241m.\u001b[39mapply()\n",
      "Cell \u001b[0;32mIn[10], line 101\u001b[0m, in \u001b[0;36mQ_learning.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m curr_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mget_current_state()\n\u001b[1;32m    100\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(curr_state)\n\u001b[0;32m--> 101\u001b[0m next_state, reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mdo_action(action)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Choose maximum Q-value for next state\u001b[39;00m\n\u001b[1;32m    103\u001b[0m max_next_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_table[next_state[\u001b[38;5;241m0\u001b[39m], next_state[\u001b[38;5;241m1\u001b[39m], next_state[\u001b[38;5;241m2\u001b[39m], next_state[\u001b[38;5;241m3\u001b[39m], :])\n",
      "Cell \u001b[0;32mIn[3], line 58\u001b[0m, in \u001b[0;36mCartPole.do_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m   \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    Performs a step in the environment. Gets the values for Observation, reward and checks if the game is over\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m        reward: Reward basing on the taken action\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m   \"\"\"\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m    new_state, reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_isTerminated, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mstep(action)       \n\u001b[1;32m     59\u001b[0m    \u001b[38;5;66;03m# Update the current state\u001b[39;00m\n\u001b[1;32m     60\u001b[0m    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currState \u001b[38;5;241m=\u001b[39m new_state\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:190\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    187\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:302\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    301\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    303\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    gamma = 0.7 # Discount rate\n",
    "    alpha = 0.1 # Learning rate\n",
    "    epsilon = 1 # How much we want to explore \n",
    "    episodes = 100000 # Number of episodes\n",
    "\n",
    "    isLearning = False # Set to False to test the trained model\n",
    "\n",
    "    cart_pole = CartPole(isLearning)\n",
    "    agent = Q_learning(cart_pole, gamma, alpha, epsilon, episodes, isLearning)\n",
    "    agent.apply()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f66ffc-410d-43a9-87d8-d36aa407ec35",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c17dc-4aeb-44d7-9f9b-f16f3b459bf8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
